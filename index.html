<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Jiahao Nie</title>
  
  <meta name="author" content="Jiahao Nie">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="image/favicon_ntu.ico">
</head>

  <body>
  <table width="950" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="70%" valign="middle">
        <p align="center">
          <name>Jiahao Nie &nbsp;&nbsp;&nbsp; 聂 嘉豪 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
		  </name>
        </p>

		<p style="line-height: 1.6em;">
      I am a Ph.D. candidate at <a href="https://www.ntu.edu.sg">Nanyang Technological University</a>, under the supervision of  <a href="https://scholar.google.com/citations?user=uYmK-A0AAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Prof. Shijian Lu</a>, <a href="https://scholar.google.com/citations?user=9sQVUMoAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Prof. Yap-Peng Tan</a>, and <a href="https://scholar.google.com/citations?user=UGZXLxIAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Prof. Alex C. Kot</a>.
      Before that, I obtained my M.S. in Signal Processing from <a href="https://www.ntu.edu.sg/"> Nanyang Technological University</a>, supervised by <a href="https://scholar.google.com/citations?user=9sQVUMoAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Prof. Yap-Peng Tan</a>, 
      and my B.ENG. in Information Engineering from <a href="https://www.nwpu.edu.cn/">Northwestern Polytechnical University</a>, supervised by <a href="https://scholar.google.com/citations?user=H39s-iIAAAAJ&hl=zh-CN">Prof. Jie Chen</a>.
    </p>  
    <p style="line-height: 1.6em;">
    My research interests lie in Computer Vision, particularly in few-shot segmentation and fine-grained understanding within MLLMs, including topics such as hallucination mitigating, video grounding, and knowledge utilization.
    Feel free to contact me via Email or WeChat for collaborations, questions, or just to chat!
    </p>


        <p align=center>
          <a href="mailto:jiahao007@e.ntu.edu.sg">Email</a> &nbsp/&nbsp
          <a href="https://github.com/niejiahao1998">Github</a> &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?user=LGM10RQAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Google Scholar</a> &nbsp/&nbsp
          <a href="image/resume.pdf">Resume</a> &nbsp/&nbsp
          <a href="image/wechat.jpg">WeChat</a>      

        </p>
      </td>
      <td width="30%">
      <img src="image/jiahaonie2.jpg" width="200">
      </td>

      </tr>
      </table>



<p></p><p></p><p></p><p></p><p></p>
<style>
.news-title {
  margin-bottom: 14px;   /* 控制标题和列表之间的距离 */
}

.news-list {
  list-style: none;
  padding: 0;
  margin: 0;
}

.news-item {
  display: flex;
  align-items: flex-start;
  margin-bottom: 6px;   /* 每一条之间的行间距 */
}

.news-dot {
  width: 18px;
  flex-shrink: 0;
  font-size: 14px;
  line-height: 1;
  margin-top: 1.5px;
}

.news-date {
  width: 88px;
  font-weight: 600;
  flex-shrink: 0;
}

.news-content {
  flex: 1;
}
</style>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
    <td width="100%" valign="middle">
      
      <div class="news-title">
        <heading>News</heading>
      </div>

      <ul class="news-list">

        <li class="news-item">
          <span class="news-dot">•</span>
          <span class="news-date">[Feb. 2026]</span>
          <span class="news-content">1 paper is accepted by CVPR 2026.</span>
        </li>

        <li class="news-item">
          <span class="news-dot">•</span>
          <span class="news-date">[Jan. 2026]</span>
          <span class="news-content">Join TeleAI as an AI algorithm intern.</span>
        </li>

        <li class="news-item">
          <span class="news-dot">•</span>
          <span class="news-date">[Nov. 2025]</span>
          <span class="news-content">1 paper is accepted by AAAI 2026.</span>
        </li>

        <li class="news-item">
          <span class="news-dot">•</span>
          <span class="news-date">[Nov. 2025]</span>
          <span class="news-content">1 paper is accepted by WACV 2026.</span>
        </li>

        <li class="news-item">
          <span class="news-dot">•</span>
          <span class="news-date">[Sep. 2025]</span>
          <span class="news-content">1 paper is accepted by NeurIPS 2025 (Oral).</span>
        </li>

        <li class="news-item">
          <span class="news-dot">•</span>
          <span class="news-date">[Feb. 2025]</span>
          <span class="news-content">1 paper is accepted by CVPR 2025.</span>
        </li>

        <li class="news-item">
          <span class="news-dot">•</span>
          <span class="news-date">[Nov. 2024]</span>
          <span class="news-content">1 paper is accepted by AAAI 2025.</span>
        </li>

        <li class="news-item">
          <span class="news-dot">•</span>
          <span class="news-date">[Sep. 2024]</span>
          <span class="news-content">1 paper is accepted by NeurIPS 2024.</span>
        </li>

        <li class="news-item">
          <span class="news-dot">•</span>
          <span class="news-date">[Sep. 2024]</span>
          <span class="news-content">Win the NTU IGP Research Excellence Award.</span>
        </li>

        <li class="news-item">
          <span class="news-dot">•</span>
          <span class="news-date">[Mar. 2024]</span>
          <span class="news-content">1 paper is accepted by ICME 2024 (Oral).</span>
        </li>

        <li class="news-item">
          <span class="news-dot">•</span>
          <span class="news-date">[Feb. 2024]</span>
          <span class="news-content">1 paper is accepted by CVPR 2024.</span>
        </li>

        <li class="news-item">
          <span class="news-dot">•</span>
          <span class="news-date">[Sep. 2023]</span>
          <span class="news-content">1 paper is accepted by NeurIPS 2023.</span>
        </li>

      </ul>

    </td>
  </tr>
</table>


<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
<tr>
  <td width="100%" valign="middle">
    <heading>Preprint & Publication <!--<a href="https://scholar.google.com/citations?user=LGM10RQAAAAJ&hl=en&oi=ao" style="font-size:22px;">[Google Scholar]</a></heading>-->
    </heading>
    &nbsp;(&nbsp;* Equal Contribution &nbsp; † Corresponding Author&nbsp;)
  </td>
</tr>
</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
    <td width="15%">
      <img src='paper/arXiv2026_CPS.png'  width="200">
    </td>
    <td valign="middle" width="75%">
    <p  style="line-height: 1.5em;">
      <strong>Boosting SAM for Cross-Domain Few-Shot Segmentation via Conditional Point Sparsification</strong></a><br>
      <strong>Jiahao Nie<sup>*</sup></strong>, <a href="https://scholar.google.com/citations?user=uOAYTXoAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Yun Xing<sup>*</sup></a>, <a href="https://scholar.google.com/citations?user=BpkQZGgAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Wenbin An</a>,
      <a href="https://scholar.google.com/citations?user=ux-dlywAAAAJ&hl=en">Qingsong Zhao</a>, <a href="https://scholar.google.com/citations?user=p26zthIAAAAJ&hl=en">Jiawei Shao</a>,
      <a href="https://scholar.google.com/citations?user=t9EqYQIAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Yap-Peng Tan</a>, <a href="https://scholar.google.com/citations?user=UGZXLxIAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Alex C. Kot</a>,
      <a href="https://scholar.google.com/citations?user=uYmK-A0AAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Shijian Lu</a>, <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=zh-CN">Xuelong Li</a><br>
      <em>arXiv	preprint arXiv:2602.05218, <strong>2026</strong>
      <a href="https://arxiv.org/pdf/2602.05218"><strong>[Paper]&nbsp</strong></a><br>
      </p>
    </td>
  </tr>
 </table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
    <td width="15%">
      <img src='paper/arXiv2026_MPA.png'  width="200">
    </td>
    <td valign="middle" width="75%">
    <p  style="line-height: 1.5em;">
      <strong>Cross-Domain Few-Shot Segmentation via Multi-view Progressive Adaptation</strong></a><br>
      <strong>Jiahao Nie<sup>*</sup></strong>, Guanqiao Fu<sup>*</sup></a>, <a href="https://scholar.google.com/citations?user=BpkQZGgAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Wenbin An</a>,
      <a href="https://scholar.google.com/citations?user=t9EqYQIAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Yap-Peng Tan</a>, <a href="https://scholar.google.com/citations?user=UGZXLxIAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Alex C. Kot</a>, <a href="https://scholar.google.com/citations?user=uYmK-A0AAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Shijian Lu</a><br>
      <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition, <strong>CVPR 2026</strong>
      <a href="https://arxiv.org/pdf/2602.05217"><strong>[Paper]&nbsp</strong></a><br>
      </p>
    </td>
  </tr>
 </table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
    <td width="15%">
      <img src='paper/arXiv2026_EMGround.png'  width="200">
    </td>
    <td valign="middle" width="75%">
    <p  style="line-height: 1.5em;">
      <strong>E.M.Ground: A Temporal Grounding Vid-LLM with Holistic Event Perception and Matching</strong></a><br>
      <strong>Jiahao Nie<sup>*</sup></strong>, <a href="https://scholar.google.com/citations?user=BpkQZGgAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Wenbin An<sup>*</sup></a>,
      <a href="https://scholar.google.com/citations?user=sRBTPp4AAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Gongjie Zhang</a>, <a href="https://scholar.google.com/citations?user=Q6AdA7oAAAAJ&hl=en">Yicheng Xu</a>,
      <a href="https://scholar.google.com/citations?user=t9EqYQIAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Yap-Peng Tan</a>, <a href="https://scholar.google.com/citations?user=UGZXLxIAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Alex C. Kot</a>, <a href="https://scholar.google.com/citations?user=uYmK-A0AAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Shijian Lu</a><br>
      <em>arXiv	preprint arXiv:2602.05215, <strong>2026</strong>
      <a href="https://arxiv.org/pdf/2602.05215"><strong>[Paper]&nbsp</strong></a><br>
      </p>
    </td>
  </tr>
 </table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
    <td width="15%">
      <img src='paper/arXiv2024_MMRel.png'  width="200">
    </td>
    <td valign="middle" width="75%">
    <p  style="line-height: 1.5em;">
      <a href="https://niejiahao1998.github.io/MMRel"><strong>MMRel: Benchmarking Relation Understanding in Multi-Modal Large Language Models</strong></a><br>
      <strong>Jiahao Nie<sup>*</sup></strong>, <a href="https://scholar.google.com/citations?user=sRBTPp4AAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Gongjie Zhang<sup>*</sup></a>,
      <a href="https://scholar.google.com/citations?user=BpkQZGgAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Wenbin An<sup>*</sup></a>, <a href="https://scholar.google.com/citations?user=uOAYTXoAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Yun Xing<sup>*</sup></a>,
      <a href="https://scholar.google.com/citations?user=t9EqYQIAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Yap-Peng Tan</a>, <a href="https://scholar.google.com/citations?user=UGZXLxIAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Alex C. Kot</a>, <a href="https://scholar.google.com/citations?user=uYmK-A0AAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Shijian Lu</a><br>
      <em>arXiv	preprint arXiv:2406.09121, <strong>2024</strong>
      <a href="https://arxiv.org/pdf/2406.09121"><strong>[Paper]&nbsp</strong></a><a href="https://niejiahao1998.github.io/MMRel"><strong>[Benchmark]</strong></a><br>
      </p>
    </td>
  </tr>
 </table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
    <td width="15%">
      <img src='paper/CVPR2024_IFA.png'  width="200">
    </td>
    <td valign="middle" width="75%">
    <p  style="line-height: 1.5em;">
      <strong>Cross-Domain Few-Shot Segmentation via Iterative Support-Query Correspondence Mining</strong><br>
      <strong>Jiahao Nie<sup>*</sup></strong>, <a href="https://scholar.google.com/citations?user=uOAYTXoAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Yun Xing<sup>*</sup></a>, <a href="https://scholar.google.com/citations?user=sRBTPp4AAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Gongjie Zhang</a>,
      <a href="https://scholar.google.com/citations?user=XnvspFEAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Pei Yan</a>, <a href="https://scholar.google.com/citations?user=yGKsEpAAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Aoran Xiao</a>,
      <a href="https://scholar.google.com/citations?user=t9EqYQIAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Yap-Peng Tan</a>, <a href="https://scholar.google.com/citations?user=UGZXLxIAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Alex C. Kot</a>, <a href="https://scholar.google.com/citations?user=uYmK-A0AAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Shijian Lu</a><br>
      <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition, <strong>CVPR 2024</strong>
      <a href="https://arxiv.org/pdf/2401.08407.pdf"><strong>[Paper]&nbsp</strong></a><a href="https://github.com/niejiahao1998/IFA"><strong>[Code]</strong></a><br>
      </p>
    </td>
  </tr>
 </table>


 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
    <td width="15%">
      <img src='paper/ICME2024_CSL.png'  width="200">
    </td>
    <td valign="middle" width="75%">
    <p  style="line-height: 1.5em;">
      <strong>Color Space Learning for Cross-Color Person Re-identification</strong><br>
      <strong>Jiahao Nie</strong>, <a href="https://scholar.google.com/citations?user=3MQDywkAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Shan Lin</a>, <a href="https://scholar.google.com/citations?user=UGZXLxIAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=ao">Alex C. Kot</a><br>
      <em>IEEE International Conference on Multimedia and Expo, <strong>ICME 2024 (Oral)</strong>
      <a href="https://arxiv.org/pdf/2405.09487"><strong>[Paper]&nbsp</strong></a><a href="https://github.com/niejiahao1998/CSL"><strong>[Code]</strong></a><br>
      </p>
    </td>
  </tr>
 </table>


 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
    <td width="15%">
      <img src='paper/AAAI2026_KCM.png'  width="200">
    </td>
    <td valign="middle" width="75%">
    <p  style="line-height: 1.5em;">
      <strong>Enhancing Retrieval-Augmented Large Vision Language Models via Knowledge Conflict Mitigation</strong><br>
      Wenbin An<sup>*</sup>, <strong>Jiahao Nie<sup>*</sup></strong>, Feng Tian, Mingxiang Cai, Yaqiang Wu, Xiaoqin Zhang, Shijian Lu<br>
      <em>AAAI Conference on Artificial Intelligence, <strong>AAAI 2026</strong>
      <a href="https://arxiv.org/"><strong>[Paper]&nbsp</strong></a><br>
      </p>
    </td>
  </tr>
 </table>



 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
    <td width="15%">
      <img src='paper/NeurIPS2025_ALFAR.png'  width="200">
    </td>
    <td valign="middle" width="75%">
    <p  style="line-height: 1.5em;">
      <strong>Boosting Knowledge Utilization in Multimodal Large Language Models via Adaptive Logits Fusion and Attention Reallocation</strong><br>
      Wenbin An<sup>*</sup>, <strong>Jiahao Nie<sup>*</sup></strong>, Feng Tian, Haonan Lin, Mingxiang Cai, Yaqiang Wu, QianYing Wang, Xiaoqin Zhang, Shijian Lu<br>
      <em>Advances in Neural Information Processing Systems, <strong>NeurIPS 2025 (Oral)</strong>
      <a href="https://openreview.net/pdf?id=qYkhCah8OZ"><strong>[Paper]&nbsp</strong></a><br>
      </p>
    </td>
  </tr>
 </table>


 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
    <td width="15%">
      <img src='paper/arxiv2025_MLLM_Tool_Survey.png'  width="200">
    </td>
    <td valign="middle" width="75%">
    <p  style="line-height: 1.5em;">
      <strong>Empowering Multimodal LLMs with External Tools: A Comprehensive Survey</strong><br>
      Wenbin An<sup>*</sup>, <strong>Jiahao Nie<sup>*</sup></strong>, Yaqiang Wu, Feng Tian, Shijian Lu, Qinghua Zheng<br>
      <em>arXiv preprint arXiv:2508.10955, <strong>2025</strong>
      <a href="https://arxiv.org/pdf/2508.10955"><strong>[Paper]&nbsp</strong></a><br>
      </p>
    </td>
  </tr>
 </table>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
    <td width="15%">
      <img src='paper/WACV2026_MTIF.png'  width="200">
    </td>
    <td valign="middle" width="75%">
    <p  style="line-height: 1.5em;">
      <strong>Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios</strong><br>
      Mingwei Tang, <strong>Jiahao Nie<sup>†</sup></strong>, Guang Yang, Ziqing Cui, Jie Li<br>
      <em>IEEE/CVF Winter Conference on Applications of Computer Vision, <strong>WACV 2026</strong>
      <a href="https://arxiv.org/pdf/2512.20556"><strong>[Paper]&nbsp</strong></a><br>
      </p>
    </td>
  </tr>
 </table>


 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
    <td width="15%">
      <img src='paper/CVPR2025_AGLA.png'  width="200">
    </td>
    <td valign="middle" width="75%">
    <p  style="line-height: 1.5em;">
      <strong>Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention</strong><br>
      Wenbin An, Feng Tian, Sicong Leng, <strong>Jiahao Nie</strong>, Haonan Lin, QianYing Wang, Ping Chen, Xiaoqin Zhang, Shijian Lu<br>
      <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition, <strong>CVPR 2025</strong>
      <a href="https://arxiv.org/pdf/2406.12718"><strong>[Paper]&nbsp</strong></a><br>
      </p>
    </td>
  </tr>
 </table>


 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
    <td width="15%">
      <img src='paper/AAAI2025_SDC.png'  width="200">
    </td>
    <td valign="middle" width="75%">
    <p  style="line-height: 1.5em;">
      <strong>Unleashing the Potential of Model Bias for Generalized Category Discovery</strong><br>
      Wenbin An, Haonan Lin, <strong>Jiahao Nie</strong>, Feng Tian, Wenkai Shi, Yaqiang Wu, Qianying Wang, Ping Chen<br>
      <em>AAAI Conference on Artificial Intelligence, <strong>AAAI 2025</strong>
      <a href="https://arxiv.org/pdf/2412.12501"><strong>[Paper]&nbsp</strong></a><br>
      </p>
    </td>
  </tr>
 </table>


 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
    <td width="15%">
      <img src='paper/NeurIPS2024_RAIL.png'  width="200">
    </td>
    <td valign="middle" width="75%">
    <p  style="line-height: 1.5em;">
      <strong>Advancing Cross-Domain Discriminability in Continual Learning of Vision-Language Models</strong><br>
      Yicheng Xu<sup>*</sup>, Yuxin Chen<sup>*</sup>, <strong>Jiahao Nie</strong>, Yusong Wang, Huiping Zhuang, Manabu Okumura<br>
      <em>Advances in Neural Information Processing Systems, <strong>NeurIPS 2024</strong>
      <a href="https://arxiv.org/pdf/2406.18868"><strong>[Paper]&nbsp</strong></a><br>
      </p>
    </td>
  </tr>
 </table>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
    <td width="15%">
      <img src='paper/arXiv2024_DKA.png'  width="200">
    </td>
    <td valign="middle" width="75%">
    <p  style="line-height: 1.5em;">
      <strong>Knowledge Acquisition Disentanglement for Knowledge-based Visual Question Answering with Large Language Models</strong><br>
      Wenbin An, Feng Tian, <strong>Jiahao Nie</strong>, Wenkai Shi, Haonan Lin, Yan Chen, QianYing Wang, Yaqiang Wu, Guang Dai, Ping Chen<br>
      <em>arXiv preprint arXiv:2407.15346, <strong>2024</strong>
      <a href="https://arxiv.org/pdf/2407.15346"><strong>[Paper]&nbsp</strong></a><br>
      </p>
    </td>
  </tr>
 </table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
    <td width="15%">
      <img src='paper/NeurIPS2023_CoCu.png'  width="200">
    </td>
    <td valign="middle" width="75%">
    <p  style="line-height: 1.5em;">
      <strong>Rewrite Caption Semantics: Bridging Semantic Gaps for Language-Supervised Semantic Segmentation</strong><br>
      Yun Xing, Jian Kang, Aoran Xiao, <strong>Jiahao Nie</strong>, Ling Shao, Shijian Lu<br>
      <em>Advances in Neural Information Processing Systems, <strong>NeurIPS 2023</strong>
      <a href="https://arxiv.org/pdf/2309.13505"><strong>[Paper]&nbsp</strong></a><br>
      </p>
    </td>
  </tr>
 </table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
<tr onmouseout="aperture_stop()" onmouseover="aperture_start()" >
  <td width="15%">
    <img src='paper/ICICSP2021_3DConv.png'  width="200">
  </td>
  <td valign="middle" width="75%">
  <p  style="line-height: 1.5em;">
    <strong>A Novel 3D Convolutional Neural Network for Action Recognition in Infrared Videos</strong><br>
    <strong>Jiahao Nie</strong>, Longbin Yan, <a href="https://scholar.google.com/citations?user=xyfMMGIAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=sra">Xiuheng Wang</a>, <a href="https://scholar.google.com/citations?user=H39s-iIAAAAJ&hl=zh-CN&inst=8669986779262753491&oi=sra">Jie Chen</a><br>
    <em>IEEE International Conference on Information Communication and Signal Processing, <strong>IEEE ICICSP 2021</strong>
    <a href="https://ieeexplore.ieee.org/abstract/document/9611896/"><strong>[Paper]</strong></a><br>
    </p>
  </td>
</tr>
</table>





      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Education</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

        <tr>
          <td width="5%">
            <img src='image/ntu_icon.jpg' width="100">
          </td>

          <td width="75%" valign="middle">
          <p style="line-height: 1.6em;">
          <stronghuge> Jan. 2023 - Present <br />
          Nanyang Technological University, &nbsp; Singapore</stronghuge><br />
          Ph.D. in Computer Sciense <br />
          </p>
        </td>
      </tr>

        <tr>
          <td width="5%">
            <img src='image/ntu_icon.jpg' width="100">
          </td>

          <td width="75%" valign="middle">
          <p style="line-height: 1.6em;">
          <stronghuge> Aug. 2021 - Aug. 2022 <br />
          Nanyang Technological University, &nbsp; Singapore</stronghuge><br />
          M.S. in Signal Processing <br />
          </p>
        </td>
      </tr>

        <tr>
          <td width="5%">
            <img src='image/nwpu_icon.jpg' width="100">
          </td>

          <td width="75%" valign="middle">
          <p style="line-height: 1.6em;">
          <stronghuge> Sep. 2016 - Jun. 2020<br />
          Northwestern Polytechnical University, &nbsp; China</stronghuge><br />
          B.ENG. in Information Engineering &nbsp; <strong>Outstanding Graduate</strong><br />
          </p>
        </td>
      </tr>    
	  
      </table>








<p></p><p></p><p></p><p></p><p></p>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research Experience</heading>
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <img src='image/rose_icon.jpg' width="100">
          </a>
          </td>

          <td width="80%" valign="middle">
          <p  style="line-height: 1.3em;">
          <stronghuge>Sep. 2021 - Dec. 2022<br />
          Rapid-Rich Object Search Lab, &nbsp; NTU</stronghuge><br />
          <huge><em>M.S. Dissertation Project & Research Associate</em></huge><br />
          Advisors: &nbsp; <a href="https://personal.ntu.edu.sg/eyptan/index.html">Prof. Yap-Peng Tan</a>,
          <a href="https://personal.ntu.edu.sg/eackot/index.html">Prof. Alex C. Kot</a>
          and <a href="https://linshanify.github.io/">Dr. Shan Lin</a>
          </p>
        </td>
      </tr>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <img src='image/ciaic_icon.jpg' width="100">
            </a>
          </td>

          <td width="80%" valign="middle">
          <p  style="line-height: 1.3em;">
          <stronghuge>Aug. 2020 - Jul. 2021<br />
          Center of Intelligent Acoustics and Immersive Communications, &nbsp; NWPU</stronghuge><br />
          <huge><em>Research  Assistant</em></huge><br />
          Advisor: <a href="http://www.jie-chen.com">Prof. Jie Chen</a>
        </td>
      </tr>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="10%">
            <img src='image/tsinghua_icon.jpg' width="100">
            </a>
          </td>

          <td width="80%" valign="middle">
          <p  style="line-height: 1.3em;">
          <stronghuge>Jul. 2019 - Aug. 2019<br />
          Navigation Technology Engineering Center Laboratory, &nbsp; THU</stronghuge><br />
          <huge><em>Summer Intern</em></huge><br />
          Advisor: <a href="http://faculty.dpi.tsinghua.edu.cn/weiqi.html">Prof. Qi Wei</a>
          </p>
        </td>
      </tr>

      
      


    <p></p><p></p><p></p><p></p><p></p>
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Miscellaneous</heading>
              <div style="line-height:25px">
              <p>
                <li> <stronghuge><strong>Reviewer:</strong></stronghuge><br/>
                  &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp TMM, CVPR, ICCV, NeurIPS, ICML, AAAI, ICLR, ICME, ACCV<br/>
                <li> <stronghuge><strong>Teaching:</strong></stronghuge><br/>
                  &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp SC2079/CE3004/CZ3004 Multi-Disciplinary Design Project, NTU, 2023 Fall, 2024 Spring<br/>
                  &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp SC1003/SC5001/CE1103/CZ1103 Introduction to Computational Thinking and Programming, NTU, 2024 Fall<br/>
                <li> <stronghuge><strong>Activity:</strong></stronghuge><br/>
                &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Vice Chairman of Student Union, School of Marine Science and Technology, NWPU&nbsp; 2017 - 2019<br/>
              </div>
            </td>
          </tr>
    </table>



<p></p><p></p><p></p><p></p><p></p>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Honor & Award</heading>
          <div style="line-height:25px">
          <p>
      <li> <stronghuge>NTU IGP Research Excellence Award (one of all Year2 students)</stronghuge> &nbsp; 2024<br/>
      <li> <stronghuge>NTU IGP Research Scholoarship</stronghuge> &nbsp; 2023 - 2027<br/>
      <li> <stronghuge>NWPU Outstanding Graduate of the University (Top 6% student)</stronghuge> &nbsp; 2020<br/>
      <li> <stronghuge>NWPU Outstanding Graduate of the School (Top 15% student)</stronghuge> &nbsp; 2020<br/>
		  <li> <stronghuge>NWPU School-level Outstanding Student Award (three times & Top 15% student)</stronghuge> &nbsp; 2017 - 2019<br/>
      <li> <stronghuge>NWPU Special Scholarships (four times & Top 8% student)</stronghuge> &nbsp; 2017 - 2019<br/>
      <li> <stronghuge>Four national awards, including China Marine Vehicle Design and Manufacture Contest Outstanding Winner</stronghuge>&nbsp; 2017 - 2019<br/>
          </p>
          </div>
        </td>
      </tr>
</table>



<p></p><p></p><p></p><p></p><p></p>
<table style="width:25%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
  <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=39eL-bl4e0ZAO8NNLk9SQyDWKruWYpMqz_pcs1QUmjA&cl=ffffff&w=a"></script>
</tbody>
</table>

<p></p><p></p><p></p><p></p><p></p>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>

      <tr>
          <td style="padding:0px">
              <p style="text-align:left;font-size:small;"> This homepage was last updated: <span id="demo"></span>.</p>
              <script>document.getElementById("demo").innerHTML = document.lastModified;</script>
          </td>
          <td>
              <p style="text-align:right;font-size:small;">
                  This awesome website template is borrowed from <a href="https://jonbarron.info/" target="_blank">Jon Barron</a>.
              </p>
          </td>
      </tr>
  </tbody>
</table>
   

</body>
</html>
